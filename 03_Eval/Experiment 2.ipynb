{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfRd7CcATZ4qoEmeDvqNXD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eV5e5RpenOuD"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import classification_report\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from datasets import load_dataset\n","\n","path = # your path here"]},{"cell_type":"code","source":["expected_answers = ['respectful', 'toxic']\n","def get_misbehavior_rate(model_answer):\n","    if model_answer in expected_answers:\n","        return \"Expected\"\n","    else:\n","        return \"Unexpected\""],"metadata":{"id":"DeaLONdwnT-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hallucination_rate(preds):\n","  \"\"\"\n","  returns the percentage of hallucinated sentences considering reference and counterfactual sentences.\n","\n","  @params:\n","  preds: df with 'results_1' and 'results_2' columns\n","  \"\"\"\n","\n","  preds['misbehavior_1'] = [get_misbehavior_rate(output) for output in preds.results_1]\n","  preds['misbehavior_2'] = [get_misbehavior_rate(output) for output in preds.results_2]\n","\n","  n_unexpected = preds[(preds['misbehavior_1'] == 'Unexpected') | (preds['misbehavior_2'] == 'Unexpected')].shape[0]\n","\n","  return (n_unexpected / preds.shape[0])*100"],"metadata":{"id":"89_iKLV8nT76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hallucination_rate_counterfactuals_wrt_original(preds):\n","  \"\"\"\n","  returns the percentage of hallucinated sentences generated by the model when\n","  it sees the counterfactuals but does not hallucinate when it sees the original sentences.\n","\n","  @params:\n","  preds: df with 'results_1' (counterfactual) and 'results_2' (original) columns\n","  \"\"\"\n","\n","  preds['misbehavior_1'] = [get_misbehavior_rate(output) for output in preds.results_1]\n","  preds['misbehavior_2'] = [get_misbehavior_rate(output) for output in preds.results_2]\n","\n","  n_unexpected = preds[(preds['misbehavior_1'] == 'Unexpected') & (preds['misbehavior_2'] == 'Expected')].shape[0]\n","\n","  return (n_unexpected / preds.shape[0])*100"],"metadata":{"id":"7f6Ce2yOnYpq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hallucination_rate_counterfactuals_only(preds):\n","  \"\"\"\n","  returns the percentage of hallucinated sentences generated by the model when it sees the counterfactuals.\n","\n","  @params:\n","  preds: df with 'results_1' (counterfactual) and 'results_2' (original) columns\n","  \"\"\"\n","\n","  preds['misbehavior_1'] = [get_misbehavior_rate(output) for output in preds.results_1]\n","  preds['misbehavior_2'] = [get_misbehavior_rate(output) for output in preds.results_2]\n","\n","  n_unexpected = preds[preds['misbehavior_1'] == 'Unexpected'].shape[0]\n","\n","  return (n_unexpected / preds.shape[0])*100"],"metadata":{"id":"n2O51WrXnYnB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def label_flip_rate(preds):\n","  \"\"\"\n","  returns the percentage of label flips.\n","\n","  @params:\n","  preds: df with 'results_1' and 'results_2' columns\n","  \"\"\"\n","\n","  preds['misbehavior_1'] = [get_misbehavior_rate(output) for output in preds.results_1]\n","  preds['misbehavior_2'] = [get_misbehavior_rate(output) for output in preds.results_2]\n","\n","  expected_rows = preds[(preds['misbehavior_1'] == 'Expected') & (preds['misbehavior_2'] == 'Expected')]\n","  num_label_flips = expected_rows[expected_rows['results_1'] != expected_rows['results_2']].shape[0]\n","  total_expected = expected_rows.shape[0]\n","\n","  return (num_label_flips / total_expected) * 100"],"metadata":{"id":"y8HMuQHAneEg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def f1_score(preds):\n","  \"\"\"\n","  returns the f1-score.\n","\n","  @params:\n","  preds: dataframe with 'results_1' and 'results_2' columns -- we only need 'results_2' because those are the non-counterfactual sentences\n","  \"\"\"\n","\n","  preds['misbehavior_2'] = [get_misbehavior_rate(output) for output in preds.results_2]\n","\n","  df = pd.read_excel(path+'data/inputs experiment 2.xlsx')\n","  df['Label'].replace({'hateful': 'toxic', 'non-hateful':'respectful'}, inplace=True)\n","\n","  df['Predicted'] = preds['results_2']\n","  df['misbehavior'] = preds['misbehavior_2']\n","  df = df[df['misbehavior']=='Expected']\n","  df.reset_index(drop=True, inplace=True)\n","\n","  true_labels = df['Label'].to_list()\n","  predicted_labels = df['Predicted'].to_list()\n","\n","  print(classification_report(true_labels, predicted_labels, zero_division=False, digits=4))"],"metadata":{"id":"eh6h1qxrneB4"},"execution_count":null,"outputs":[]}]}