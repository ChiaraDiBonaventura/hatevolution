{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOxpUmrXVwFjkRiYq95anXk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cfEdVIvfc9MK"},"outputs":[],"source":["import pandas as pd\n","from sklearn.metrics import classification_report\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import bitsandbytes as bnb\n","\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, AutoModelForCausalLM, GenerationConfig\n","\n","import openai\n","openai.api_key = # your OpenAI key here\n","\n","\n","from googleapiclient import discovery\n","import json\n","import time\n","API_KEY = # your Perspective key here\n","client = discovery.build(\n","  \"commentanalyzer\",\n","  \"v1alpha1\",\n","  developerKey=API_KEY,\n","  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n","  static_discovery=False,\n",")\n","\n","path = # your path here"]},{"cell_type":"code","source":["dataset = # dataset\n","df = pd.read_csv(path+dataset)"],"metadata":{"id":"hMpQMZ7qd1jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing Seq2Seq models and running the inference"],"metadata":{"id":"_irsdPOZgy-T"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"declare-lab/flan-alpaca-xl\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\"declare-lab/flan-alpaca-xl\",\n","                                              torch_dtype=\"auto\",\n","                                              device_map=\"auto\",\n","                                              load_in_8bit=True\n","                                              )"],"metadata":{"id":"viCPjfXKeejy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_template = \"Classify the input text as 'respectful' or 'toxic'.\\nInput text: {text}.\\nAnswer: \"\n","label_map = {\n","        \"toxic\": 'toxic',\n","        \"'toxic'\": 'toxic',\n","        \"Toxic\": 'toxic',\n","        \"'Toxic'\": 'toxic',\n","        \"respectful\": 'respectful',\n","        \"'respectful'\": 'respectful',\n","        \"Respectful\": 'respectful',\n","        \"'Respectful'\": 'respectful'\n","    }\n","\n","batch_size = 100  # You can adjust this based on your available GPU memory\n","predicted_labels_1 = []\n","predicted_labels_2 = []\n","\n","total_rows = len(df)\n","num_batches = (total_rows + batch_size - 1) // batch_size\n","\n","for batch_num in tqdm(range(num_batches)):\n","    start_idx = batch_num * batch_size\n","    end_idx = min((batch_num + 1) * batch_size, total_rows)\n","\n","    batch_df = df.iloc[start_idx:end_idx]\n","\n","    for index, row in batch_df.iterrows():\n","\n","        text1 = row['Sentence 1']\n","        text2 = row['Sentence 2']\n","\n","        prompt = prompt_template.format(text=text1)\n","        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n","        outputs = model.generate(input_ids, max_length=20)\n","        predicted_label_1 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        standardized_label_1 = label_map.get(predicted_label_1, predicted_label_1)\n","        predicted_labels_1.append(standardized_label_1)\n","\n","        prompt = prompt_template.format(text=text2)\n","        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n","        outputs = model.generate(input_ids, max_length=20)\n","        predicted_label_2 = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        standardized_label_2 = label_map.get(predicted_label_2, predicted_label_2)\n","        predicted_labels_2.append(standardized_label_2)\n","\n","    torch.cuda.empty_cache()"],"metadata":{"id":"JvEo0mSnjHrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = # predictions/dataset_model.csv\n","ids = df['id'].tolist()\n","\n","pd.DataFrame({'id':ids, 'results':predicted_labels}).to_csv(path+filename, index=False)"],"metadata":{"id":"qPdruFLWPHuD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing SeqClassification models and run inference"],"metadata":{"id":"NONGuuU97kch"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r2-target\")\n","model = AutoModelForSequenceClassification.from_pretrained(\"facebook/roberta-hate-speech-dynabench-r2-target\",\n","                                                           torch_dtype=\"auto\",\n","                                                           device_map=\"auto\").to(\"cuda\")"],"metadata":{"id":"0jxpj9hE-9JL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size = 100  # You can adjust this based on your available GPU memory\n","predicted_labels_1 = []\n","predicted_labels_2 = []\n","\n","total_rows = len(df)\n","num_batches = (total_rows + batch_size - 1) // batch_size\n","\n","for batch_num in tqdm(range(num_batches)):\n","    start_idx = batch_num * batch_size\n","    end_idx = min((batch_num + 1) * batch_size, total_rows)\n","\n","    batch_df = df.iloc[start_idx:end_idx]\n","\n","    for index, row in batch_df.iterrows():\n","\n","        text1 = row['Sentence 1']\n","        text2 = row['Sentence 2']\n","\n","        # sentence 1\n","        inputs = tokenizer(text1, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        predicted_label_idx = torch.argmax(outputs.logits, dim=1).item()\n","        predicted_label_1 = label_map[predicted_label_idx]\n","        predicted_labels_1.append(predicted_label_1)\n","\n","        # sentence 2\n","        inputs = tokenizer(text2, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        predicted_label_idx = torch.argmax(outputs.logits, dim=1).item()\n","        predicted_label_2 = label_map[predicted_label_idx]\n","        predicted_labels_2.append(predicted_label_2)\n","\n","    torch.cuda.empty_cache()"],"metadata":{"id":"9wpkV96XjPBo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = # predictions/dataset_model.csv\n","ids = df['id'].tolist()\n","\n","pd.DataFrame({'id':ids, 'results':predicted_labels}).to_csv(path+filename, index=False)"],"metadata":{"id":"ZRd054Bn7oJa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing OpenAI models and run inference"],"metadata":{"id":"gH3R3UFRLzdr"}},{"cell_type":"code","source":["def classify_hate_speech(texts, model):\n","\n","  prompt_template = (\n","            \"Classify the input text as 'respectful' or 'toxic'.\\n\"\n","            \"Input text: {text}.\\n\"\n","            \"Answer with only one word, 'respectful' or 'toxic': \")\n","\n","  responses = []\n","  for text in tqdm(texts):\n","    prompt = prompt_template.format(text=text)\n","    try:\n","      response = openai.ChatCompletion.create(\n","                model=model,\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in text classification.\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                max_tokens=10)\n","\n","      label = response['choices'][0]['message']['content'].strip()\n","      responses.append(label)\n","    except Exception as e:\n","      print(f\"Error processing text: {text}\\n{e}\")\n","      responses.append(\"Error\")\n","\n","  return responses"],"metadata":{"id":"-tGxzeddGhLu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_1 = df['Sentence 1'].tolist()\n","classifications_1 = classify_hate_speech(texts_1, model=\"gpt-3.5-turbo\")"],"metadata":{"id":"lgk_iEG4jbHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_2 = df['Sentence 2'].tolist()\n","classifications_2 = classify_hate_speech(texts_2, model=\"gpt-3.5-turbo\")"],"metadata":{"id":"NEPEcHM_jdsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_map = {\n","        \"toxic\": 'toxic',\n","        \"'toxic'\": 'toxic',\n","        \"Toxic\": 'toxic',\n","        \"'Toxic'\": 'toxic',\n","        \"respectful\": 'respectful',\n","        \"'respectful'\": 'respectful',\n","        \"Respectful\": 'respectful',\n","        \"'Respectful'\": 'respectful'\n","    }\n","\n","\n","predicted_labels_1 = []\n","for classification in classifications_1:\n","  standardized_label = label_map.get(classification, classification)\n","  predicted_labels_1.append(standardized_label)\n","\n","predicted_labels_2 = []\n","for classification in classifications_2:\n","  standardized_label = label_map.get(classification, classification)\n","  predicted_labels_2.append(standardized_label)"],"metadata":{"id":"UhsHVp9ajdkb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = # predictions/dataset_model.csv\n","ids = df['id'].tolist()\n","\n","pd.DataFrame({'id':ids, 'results':predicted_labels}).to_csv(path+filename, index=False)"],"metadata":{"id":"rcqS7fN50wo6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Open AI Moderation API"],"metadata":{"id":"bS7F3TH5QkzR"}},{"cell_type":"code","source":["def classify_hate_speech(texts, model=\"text-moderation-latest\"):\n","    prompt_template = (\n","            \"Classify the input text as 'respectful' or 'toxic'.\\n\"\n","            \"Input text: {text}.\\n\"\n","            \"Answer with only one word, 'respectful' or 'toxic': \"\n","        )\n","\n","    responses = []\n","    for text in tqdm(texts):\n","\n","        prompt = prompt_template.format(text=text)\n","\n","        try:\n","            response = openai.Moderation.create(\n","                model=model,\n","                input=prompt\n","                )\n","\n","            label = response['results'][0]['flagged']\n","            responses.append(label)\n","        except Exception as e:\n","            print(f\"Error processing text: {text}\\n{e}\")\n","            responses.append(\"Error\")\n","\n","    return responses"],"metadata":{"id":"r068kokJR7OA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_1 = df['Sentence 1'].tolist()\n","classifications_1 = classify_hate_speech(texts_1)"],"metadata":{"id":"clLpqsC_jpoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_2 = df['Sentence 2'].tolist()\n","classifications_2 = classify_hate_speech(texts_2)"],"metadata":{"id":"-80EXy7CjplY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_map = {\n","    False: \"respectful\",\n","    True: \"toxic\"\n","    }\n","\n","predicted_labels_1 = []\n","for classification in classifications_1:\n","  standardized_label = label_map.get(classification, classification)\n","  predicted_labels_1.append(standardized_label)\n","\n","predicted_labels_2 = []\n","for classification in classifications_2:\n","  standardized_label = label_map.get(classification, classification)\n","  predicted_labels_2.append(standardized_label)"],"metadata":{"id":"Ie2YRw8pjpiG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = # predictions/dataset_model.csv\n","ids = df['id'].tolist()\n","\n","pd.DataFrame({'id':ids, 'results':predicted_labels}).to_csv(path+filename, index=False)"],"metadata":{"id":"XE5zWbKoS9aZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Perspective API"],"metadata":{"id":"fLr7hvozX2oz"}},{"cell_type":"code","source":["def classify_hate_speech(texts, language='english'):\n","\n","  responses = []\n","  for text in tqdm(texts):\n","\n","    analyze_request = {'comment': { 'text': text },\n","                       'languages': ['en'],\n","                       'requestedAttributes': {'TOXICITY': {}}\n","                       }\n","\n","    try:\n","        response = client.comments().analyze(body=analyze_request).execute()\n","        time.sleep(1)\n","    except:\n","        responses.append('respectful')\n","    else:\n","        hate_speech_value = response['attributeScores']['TOXICITY']['summaryScore']['value']\n","\n","        if hate_speech_value > 0.5:\n","            responses.append('toxic')\n","        else:\n","            responses.append('respectful')\n","\n","  return responses"],"metadata":{"id":"CXyUOEAYtuCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["texts_1 = df['Sentence 1'].tolist()\n","classifications_1 = classify_hate_speech(texts_1)\n","\n","texts_2 = df['Sentence 2'].tolist()\n","classifications_2 = classify_hate_speech(texts_2)"],"metadata":{"id":"OUMrVLe5iQW2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = # predictions/dataset_model.csv\n","ids = df['id'].tolist()\n","\n","pd.DataFrame({'id':ids, 'results':classifications}).to_csv(path+filename, index=False)"],"metadata":{"id":"IFdnhKkfupcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Importing DeepSeek"],"metadata":{"id":"6PdZZLPXf8xc"}},{"cell_type":"code","source":["model_name = \"deepseek-ai/deepseek-llm-7b-base\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n","model.generation_config = GenerationConfig.from_pretrained(model_name)\n","model.generation_config.pad_token_id = model.generation_config.eos_token_id"],"metadata":{"id":"H8EnTcSYiOOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt_template = \"Classify the input text as 'respectful' or 'toxic'.\\nInput text: {text}.\\nAnswer: \"\n","\n","batch_size = 100  # You can adjust this based on your available GPU memory\n","predicted_labels_1 = []\n","predicted_labels_2 = []\n","\n","total_rows = len(df)\n","num_batches = (total_rows + batch_size - 1) // batch_size\n","\n","for batch_num in tqdm(range(num_batches)):\n","    start_idx = batch_num * batch_size\n","    end_idx = min((batch_num + 1) * batch_size, total_rows)\n","\n","    batch_df = df.iloc[start_idx:end_idx]\n","\n","    for index, row in batch_df.iterrows():\n","\n","        text1 = row['Sentence 1']\n","        text2 = row['Sentence 2']\n","\n","        prompt = prompt_template.format(text=text1)\n","        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n","        outputs = model.generate(input_ids, max_new_tokens=20)\n","        generated_tokens = outputs[0][input_ids.shape[1]:]  # decode only the newly generated tokens, ignore the prompt tokens\n","        predicted_label_1 = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","        if 'toxic' in predicted_label_1.lower():\n","          standardized_label_1 = 'toxic'\n","        elif 'respectful' in predicted_label_1.lower():\n","          standardized_label_1 = 'respectful'\n","        else:\n","          standardized_label_1 = predicted_label_1\n","\n","        predicted_labels_1.append(standardized_label_1)\n","\n","\n","        prompt = prompt_template.format(text=text2)\n","        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n","        outputs = model.generate(input_ids, max_new_tokens=20)\n","        generated_tokens = outputs[0][input_ids.shape[1]:]  # decode only the newly generated tokens, ignore the prompt tokens\n","        predicted_label_2 = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","        if 'toxic' in predicted_label_2.lower():\n","          standardized_label_2 = 'toxic'\n","        elif 'respectful' in predicted_label_2.lower():\n","          standardized_label_2 = 'respectful'\n","        else:\n","          standardized_label_2 = predicted_label_2\n","\n","        predicted_labels_2.append(standardized_label_2)\n","\n","    torch.cuda.empty_cache()"],"metadata":{"id":"0CC_RqoHj7vV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = # predictions/dataset_model.csv\n","ids = df['id'].tolist()\n","\n","pd.DataFrame({'id':ids, 'results':predicted_labels}).to_csv(path+filename, index=False)"],"metadata":{"id":"eX9jX-fzhGoS"},"execution_count":null,"outputs":[]}]}